{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5731c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing nescssary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "794aea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data from input.xlsx\n",
    "data = pd.read_excel('Input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aed77d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting arcticle text from URL\n",
    "def extract_article_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "   \n",
    "    title_element = soup.find('h1')\n",
    "    article_elements = soup.find_all('p')\n",
    "\n",
    "    \n",
    "    title = title_element.text.strip() if title_element else ''\n",
    "    article_text = ' '.join([element.text.strip() for element in article_elements])\n",
    "\n",
    "    return title, article_text\n",
    "\n",
    "#Looping through URLs for extraction\n",
    "for index, row in data.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "   \n",
    "    title, article_text = extract_article_text(url)\n",
    "\n",
    " # Saving extracted article to text file\n",
    "    filename = f'{url_id}.txt'\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(f'Title: {title}\\n\\n')\n",
    "        file.write(article_text)\n",
    "\n",
    "    print(f'Article extracted and saved: {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5af6bd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ABUTHALIB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ABUTHALIB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textual analysis completed and output saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "# Read the extracted article texts\n",
    "df = pd.read_excel('Output Data Structure.xlsx')\n",
    "\n",
    "# Initialize NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to compute variables for a given text\n",
    "def compute_variables(text):\n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Compute the number of sentences\n",
    "    num_sentences = len(sentences)\n",
    "\n",
    "    # Compute the number of words\n",
    "    num_words = len(words)\n",
    "\n",
    "    # Compute the average sentence length\n",
    "    avg_sentence_length = num_words / num_sentences\n",
    "\n",
    "    # Remove stopwords and punctuation from the words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word.lower() for word in words if word.lower() not in stop_words and word not in string.punctuation]\n",
    "\n",
    "    # Compute the number of unique words\n",
    "    num_unique_words = len(set(words))\n",
    "\n",
    "    # Compute the word frequency\n",
    "    word_freq = nltk.FreqDist(words)\n",
    "    most_common_words = word_freq.most_common(3)\n",
    "\n",
    "    # Compute the named entities using spaCy\n",
    "    doc = nlp(text)\n",
    "    named_entities = list(set([ent.text for ent in doc.ents if ent.label_ in ['ORG', 'PERSON']]))\n",
    "\n",
    "    return num_sentences, num_words, avg_sentence_length, num_unique_words, most_common_words, named_entities\n",
    "\n",
    "# Compute variables for each article text\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    filename = f'{url_id}.txt'\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        article_text = file.read()\n",
    "\n",
    "    # Compute variables for the article text\n",
    "    num_sentences, num_words, avg_sentence_length, num_unique_words, most_common_words, named_entities = compute_variables(article_text)\n",
    "\n",
    "    # Update the dataframe with the computed variables\n",
    "    df.at[index, 'Num_sentences'] = num_sentences\n",
    "    df.at[index, 'Num_words'] = num_words\n",
    "    df.at[index, 'Avg_sentence_length'] = avg_sentence_length\n",
    "    df.at[index, 'Num_unique_words'] = num_unique_words\n",
    "    df.at[index, 'Most_common_words'] = ', '.join([word[0] for word in most_common_words])\n",
    "    df.at[index, 'Named_entities'] = ', '.join(named_entities)\n",
    "\n",
    "# Save the dataframe with computed variables to the output file\n",
    "df.to_excel('Output Data Structure.xlsx', index=False)\n",
    "\n",
    "print('Textual analysis completed and output saved.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58fc3cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Number of Words Per Sentence: 5.5\n",
      "Complex Word Count: 6\n",
      "Word Count: 11\n"
     ]
    }
   ],
   "source": [
    "#analysis  of readability\n",
    "import nltk\n",
    "\n",
    "# Example usage\n",
    "text = \"This is a sample sentence. It has multiple words.\"\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# Average Number of Words Per Sentence\n",
    "avg_words_per_sentence = len(words) / len(sentences)\n",
    "print(\"Average Number of Words Per Sentence:\", avg_words_per_sentence)\n",
    "\n",
    "# Complex Word Count\n",
    "complex_word_count = sum([1 for word in words if len(word) >= 3])\n",
    "print(\"Complex Word Count:\", complex_word_count)\n",
    "\n",
    "# Word Count\n",
    "word_count = len(words)\n",
    "print(\"Word Count:\", word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44fe556e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal Pronouns Count: 2\n"
     ]
    }
   ],
   "source": [
    "#Personal Pronouns:\n",
    "import nltk\n",
    "\n",
    "# Example usage\n",
    "text = \"He went to the store. She bought a new car.\"\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# Personal Pronouns\n",
    "personal_pronouns = ['I', 'you', 'he', 'she', 'it', 'we', 'they']  # Add more personal pronouns as needed\n",
    "personal_pronoun_count = sum([1 for word in words if word.lower() in personal_pronouns])\n",
    "print(\"Personal Pronouns Count:\", personal_pronoun_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a5242c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eb4748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
